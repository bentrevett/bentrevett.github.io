{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c927534",
   "metadata": {},
   "source": [
    "# Why is the softmax function \"off by one\"?\n",
    "\n",
    "Working through the [Attention is Off by One](https://www.evanmiller.org/attention-is-off-by-one.html) articles which was interesting.\n",
    "\n",
    "Softmax is a function which converts a real-valued vector into a probability vector. It's used to select a choice from a vector. It's given by:\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "We can write this in code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad4a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(xs: list):\n",
    "    return np.exp(xs) / np.sum(np.exp(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d5f76",
   "metadata": {},
   "source": [
    "We can now convert a given list of numbers into probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7af8b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04201007, 0.84379473, 0.1141952 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([2, 5, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb094f51",
   "metadata": {},
   "source": [
    "These probabilities all sum to one (or close enough):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96aaddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([2, 5, 3]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf08b54",
   "metadata": {},
   "source": [
    "One issue with the above implementation is that $e$ to the power of a large number tends towards infinity. This means you get infinities in both the numerator and denominator which give you `nan` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee5c7701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48829/4267736518.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(xs) / np.sum(np.exp(xs))\n",
      "/tmp/ipykernel_48829/4267736518.py:4: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(xs) / np.sum(np.exp(xs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1_000, 1_000, 1_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298ed22",
   "metadata": {},
   "source": [
    "We can solve this by scaling the vector using the maximum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58722e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(xs: list):\n",
    "    e_x = np.exp(xs - np.max(xs))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c39e3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04201007, 0.84379473, 0.1141952 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([2, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae8e2763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1_000, 1_000, 1_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3e7f4",
   "metadata": {},
   "source": [
    "To prove this is equal to the initial softmax formula, we have: \n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$$\n",
    "\n",
    "Using the property:\n",
    "\n",
    "$$a^{b-c} = \\frac{a^b}{a^c}$$\n",
    "\n",
    "and also acknowledging that $\\max(x)$ is a constant, we get:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{softmax}(x)_i &= \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}\\\\[6pt]\n",
    "&=\\frac{\\frac{e^{x_i}}{e^{\\max(x)}}}{\\sum\\frac{e^{x_j}}{e^{\\max(x)}}}\\\\[6pt]\n",
    "&=\\frac{e^{x_i}}{\\frac{1}{e^{\\max(x)}}\\sum\\frac{e^{x_j}}{e^{\\max(x)}}}\\\\[6pt]\n",
    "&=\\frac{e^{x_i}}{\\sum_j e^{x_j}}\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff65e3",
   "metadata": {},
   "source": [
    "The problem raised in the [Attention is Off by One](https://www.evanmiller.org/attention-is-off-by-one.html) article is that softmax always has to make a choice of which element to pick, even if it doesn't want to. It can't return a zero vector and pick nothing.\n",
    "\n",
    "The solution? Add a $+1$ to the denominator in order to allow the softmax function to output a zero vector if all the real-valued elements are large negative numbers.\n",
    "\n",
    "$$\\text{softmax}_1(x)_i = \\frac{e^{x_i}}{1 + \\sum_j e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3425d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_one(xs: list):\n",
    "    return np.exp(xs) / (1 + np.sum(np.exp(xs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f664d56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_one([-1000, -1000, -1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925550b",
   "metadata": {},
   "source": [
    "The above implementation has the same numerical instability issues as with the initial implementation of softmax. The stable implementation, which I've taken from [here](https://github.com/google/flaxformer/blob/main/flaxformer/components/attention/dense_attention.py#L50-L50) is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba54309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_one(xs: list):\n",
    "    m = np.max([np.max(xs), 0])\n",
    "    e_x = np.exp(xs - m)\n",
    "    return e_x / (e_x.sum() + np.exp(-m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9955d",
   "metadata": {},
   "source": [
    "This allows the vector to tend to zero when they're all large negative numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c343bd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_one([-1000, -1000, -1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951e47f",
   "metadata": {},
   "source": [
    "And also have the numerical stability to work with large positive numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2726360d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_one([1_000, 1_000, 1_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936c3db",
   "metadata": {},
   "source": [
    "What's the point of a function that returns a zero vector? The $\\text{softmax}_1$ is designed to be used in the attention calculation of a transformer model, allowing it to pay attention to nothing. The transformer architecture uses residual connections, which allow the previous layer values to be passed unaltered if the $\\text{softmax}_1$ function is outputting a zero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
