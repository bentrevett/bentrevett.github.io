{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ca940a",
   "metadata": {},
   "source": [
    "# How to Use Open AI's Function Calling\n",
    "\n",
    "The latest version of Open AI's GPT models have the ability to do [\"function calling\"](https://openai.com/blog/function-calling-and-other-api-updates).\n",
    "\n",
    "First of all, what's function calling? The best way to answer this is with an example problem. How would you extract information (name, age, location) from a person introducing themselves?\n",
    "\n",
    "You could do it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5442b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7iU9ssXQMbVpOQWXiPrsf0uUcSVaR at 0x7fdd004ca770> JSON: {\n",
       "  \"id\": \"chatcmpl-7iU9ssXQMbVpOQWXiPrsf0uUcSVaR\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1690836716,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"{\\n  \\\"name\\\": \\\"Ben\\\",\\n  \\\"age\\\": 100,\\n  \\\"location\\\": \\\"London\\\"\\n}\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 60,\n",
       "    \"completion_tokens\": 23,\n",
       "    \"total_tokens\": 83\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "MODEL_NAME = \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an information extraction assistant.\n",
    "For each message, extract the: name, age and location.\n",
    "Your replies should be in the JSON format.\"\"\"\n",
    "\n",
    "MESSAGES = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"Hi. My name is Ben. I am 100 years old, and I live in London.\"},\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=MESSAGES,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43283a84",
   "metadata": {},
   "source": [
    "The above works for our sample problem, but you may find (with more complex user input or more complex information to extract) that the model will not generate valid JSON.\n",
    "\n",
    "Function calling allows us to coerce the model into generating valid JSON by using a defined JSON schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f567b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_INFORMATION_FUNCTION_SCHEMA = {\n",
    "    \"name\": \"get_user_information\",\n",
    "    \"description\": \"Extract the user's name, age, and location from their input.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user's name.\",\n",
    "            },\n",
    "            \"age\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The user's age.\"\n",
    "            },\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user's location\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"name\", \"age\", \"location\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=MESSAGES,\n",
    "    functions=[USER_INFORMATION_FUNCTION_SCHEMA],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567d355",
   "metadata": {},
   "source": [
    "One thing to note is that when using function calling, a function is only called when the model decides to use it.\n",
    "\n",
    "When the model **does** decide to use to use a function, the returned `content` in the response is `null`, and instead the function calling result is under the `function_call` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d145ae03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7iU9tl0sxwrI9lNFjtDSCZXbon6TA at 0x7fdcdbeeba40> JSON: {\n",
       "  \"id\": \"chatcmpl-7iU9tl0sxwrI9lNFjtDSCZXbon6TA\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1690836717,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": null,\n",
       "        \"function_call\": {\n",
       "          \"name\": \"get_user_information\",\n",
       "          \"arguments\": \"{\\n  \\\"name\\\": \\\"Ben\\\",\\n  \\\"age\\\": 100,\\n  \\\"location\\\": \\\"London\\\"\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"finish_reason\": \"function_call\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 129,\n",
       "    \"completion_tokens\": 30,\n",
       "    \"total_tokens\": 159\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed13a1a",
   "metadata": {},
   "source": [
    "We can then nicely parse the information from the function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddc6a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Ben', 'age': 100, 'location': 'London'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79179a8",
   "metadata": {},
   "source": [
    "When using function calling, we don't need to use the system prompt to tell the model to extract information or to JSON format the result, the model calls the appropriate function and does this for us. (Notice how we remove the system prompt telling the model to extract information from the user.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65bb446b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7iU9vQ7L0dzeSt6TkIWx1wBIuDMwe at 0x7fdd004ca450> JSON: {\n",
       "  \"id\": \"chatcmpl-7iU9vQ7L0dzeSt6TkIWx1wBIuDMwe\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1690836719,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": null,\n",
       "        \"function_call\": {\n",
       "          \"name\": \"get_user_information\",\n",
       "          \"arguments\": \"{\\n  \\\"name\\\": \\\"Ben\\\",\\n  \\\"age\\\": 100,\\n  \\\"location\\\": \\\"London\\\"\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"finish_reason\": \"function_call\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 100,\n",
       "    \"completion_tokens\": 30,\n",
       "    \"total_tokens\": 130\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi. My name is Ben. I am 100 years old, and I live in London.\"},\n",
    "    ],\n",
    "    functions=[USER_INFORMATION_FUNCTION_SCHEMA],\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87177966",
   "metadata": {},
   "source": [
    "If the model decides a function shouldn't be called, it simply returns the `content` as normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c1528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7iU9wmChjvfWxirgXfFoQHWDkcbGb at 0x7fdcf0446540> JSON: {\n",
       "  \"id\": \"chatcmpl-7iU9wmChjvfWxirgXfFoQHWDkcbGb\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1690836720,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Hello! How can I assist you today?\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 83,\n",
       "    \"completion_tokens\": 10,\n",
       "    \"total_tokens\": 93\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Howdy!\"},\n",
    "    ],\n",
    "    functions=[USER_INFORMATION_FUNCTION_SCHEMA],\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ca356",
   "metadata": {},
   "source": [
    "If you have multiple schemas, the model can only use one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e82ce512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7iU9xE90cQsMtUjUOFucpzJBLZJTn at 0x7fdd02db8810> JSON: {\n",
       "  \"id\": \"chatcmpl-7iU9xE90cQsMtUjUOFucpzJBLZJTn\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1690836721,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": null,\n",
       "        \"function_call\": {\n",
       "          \"name\": \"get_user_name\",\n",
       "          \"arguments\": \"{\\n  \\\"name\\\": \\\"Ben\\\"\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"finish_reason\": \"function_call\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 101,\n",
       "    \"completion_tokens\": 16,\n",
       "    \"total_tokens\": 117\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_schemas = [\n",
    "    {\n",
    "        \"name\": \"get_user_age\",\n",
    "        \"description\": \"Extract the user's age.\",\n",
    "        \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"age\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The user's age.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"age\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_user_name\",\n",
    "        \"description\": \"Extract the user's name.\",\n",
    "        \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user's name.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"name\"],\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi. My name is Ben. I am 100 years old, and I live in London.\"},\n",
    "    ],\n",
    "    functions=function_schemas,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99de2db",
   "metadata": {},
   "source": [
    "We can force a model to use a specific function by setting `function_call` to `{\"name\": \"<insert-function-name>\"}`. This can sometimes cause hallucinations, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bbf15f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7iU9zfmf1Qu5NYWXoOv0mEHnCSMns at 0x7fdd005332c0> JSON: {\n",
       "  \"id\": \"chatcmpl-7iU9zfmf1Qu5NYWXoOv0mEHnCSMns\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1690836723,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": null,\n",
       "        \"function_call\": {\n",
       "          \"name\": \"get_user_information\",\n",
       "          \"arguments\": \"{\\n  \\\"name\\\": \\\"John Doe\\\",\\n  \\\"age\\\": 25,\\n  \\\"location\\\": \\\"New York\\\"\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 90,\n",
       "    \"completion_tokens\": 25,\n",
       "    \"total_tokens\": 115\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Howdy!\"},\n",
    "    ],\n",
    "    functions=[USER_INFORMATION_FUNCTION_SCHEMA],\n",
    "    function_call={\"name\": \"get_user_information\"}\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03affe8b",
   "metadata": {},
   "source": [
    "The default `function_call` is `\"auto\"`, where the model itself decides which function (if any) to use. We can also set `function_call` to `\"none\"` to prevent it from using any function calls.\n",
    "\n",
    "Some other things we can do in function calling is to use `array`, `items` and `enum`.\n",
    "\n",
    "If we want the returned JSON to be a list, we give it a type of `array`. We can then define the elements of the array using the `items` field. `enum` allows us to specify that returned elements have to be selected from given values. Note: when using `enum` on an `array`, the `enum` must be part of the `items`, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0702f35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7iUA0h8e0vHhs2FLqqh9L7QVTdrlV at 0x7fdcf0057400> JSON: {\n",
       "  \"id\": \"chatcmpl-7iUA0h8e0vHhs2FLqqh9L7QVTdrlV\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1690836724,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": null,\n",
       "        \"function_call\": {\n",
       "          \"name\": \"get_discussed_shoe_features\",\n",
       "          \"arguments\": \"{\\n  \\\"features\\\": [\\\"shoe_color\\\", \\\"shoe_cost\\\"]\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 106,\n",
       "    \"completion_tokens\": 16,\n",
       "    \"total_tokens\": 122\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Howdy! I'd like some blue shoes, ideally under $100.\"},\n",
    "    ],\n",
    "    functions=[{\n",
    "        \"name\": \"get_discussed_shoe_features\",\n",
    "        \"description\": \"Extract the shoe features in the conversation.\",\n",
    "        \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"features\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"The features discussed in the conversation.\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"shoe_size\", \"shoe_color\", \"shoe_style\", \"shoe_cost\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"features\"],\n",
    "        }\n",
    "    }],\n",
    "    function_call={\"name\": \"get_discussed_shoe_features\"}\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43055d",
   "metadata": {},
   "source": [
    "We can also define the function schemas using the `pydantic` library to construct JSON schemas.\n",
    "\n",
    "Here's how we'd construct our initial `get_user_information` function schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "036b5974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'name': {'description': \"The user's name.\",\n",
       "   'title': 'Name',\n",
       "   'type': 'string'},\n",
       "  'age': {'description': \"The user's age.\", 'title': 'Age', 'type': 'integer'},\n",
       "  'location': {'description': \"The user's location\",\n",
       "   'title': 'Location',\n",
       "   'type': 'string'}},\n",
       " 'required': ['name', 'age', 'location'],\n",
       " 'title': 'UserInformationParams',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic\n",
    "\n",
    "class UserInformationParams(pydantic.BaseModel):\n",
    "    name: str = pydantic.Field(..., description=\"The user's name.\")\n",
    "    age: int = pydantic.Field(..., description=\"The user's age.\")\n",
    "    location: str = pydantic.Field(..., description=\"The user's location\")\n",
    "        \n",
    "UserInformationParams.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fda96b",
   "metadata": {},
   "source": [
    "For the `get_discussed_shoe_features` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5d270e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'features': {'description': 'The features discussed in the conversation.',\n",
       "   'items': {'enum': ['shoe_size', 'shoe_color', 'shoe_style', 'shoe_cost'],\n",
       "    'type': 'string'},\n",
       "   'title': 'Features',\n",
       "   'type': 'array'}},\n",
       " 'required': ['features'],\n",
       " 'title': 'DiscussedShoeFeaturesParams',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing\n",
    "\n",
    "shoe_features = [\n",
    "    \"shoe_size\",\n",
    "    \"shoe_color\",\n",
    "    \"shoe_style\",\n",
    "    \"shoe_cost\",\n",
    "]\n",
    "\n",
    "class DiscussedShoeFeaturesParams(pydantic.BaseModel):\n",
    "    features: list[\n",
    "        typing.Literal[tuple(shoe_features)]\n",
    "    ] = pydantic.Field(..., description=\"The features discussed in the conversation.\")\n",
    "        \n",
    "DiscussedShoeFeaturesParams.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b516f8a",
   "metadata": {},
   "source": [
    "The schemas can be used like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a7988b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7iUA1QDHUkqJB0r53ripKnIIByowx at 0x7fdcf003d360> JSON: {\n",
       "  \"id\": \"chatcmpl-7iUA1QDHUkqJB0r53ripKnIIByowx\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1690836725,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": null,\n",
       "        \"function_call\": {\n",
       "          \"name\": \"get_discussed_shoe_features\",\n",
       "          \"arguments\": \"{\\n  \\\"features\\\": [\\\"shoe_color\\\", \\\"shoe_cost\\\"]\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 106,\n",
       "    \"completion_tokens\": 16,\n",
       "    \"total_tokens\": 122\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Howdy! I'd like some blue shoes, ideally under $100.\"},\n",
    "    ],\n",
    "    functions=[{\n",
    "        \"name\": \"get_discussed_shoe_features\",\n",
    "        \"description\": \"Extract the shoe features in the conversation.\",\n",
    "        \"parameters\": DiscussedShoeFeaturesParams.model_json_schema(), \n",
    "    }],\n",
    "    function_call={\"name\": \"get_discussed_shoe_features\"}\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c554fe72",
   "metadata": {},
   "source": [
    "Note: as far as I've seen, using function calling **always** generates valid JSON. However, the JSON generated does not always have all fields, even if they are listed as required. Your code should handle the case where fields are missing, e.g. parse into JSON and call `.get` with a default value to insert if the field was not found."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
