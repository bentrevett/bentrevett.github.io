<!DOCTYPE html>
<html>
    <head>
        <link rel="stylesheet" type="text/css" href="../styles.css">
        <title>Ben Trevett</title>
    </head>
    <body>
        <!-- begin header -->
        <table>
            <td style="text-align:left">
                <b>Ben Trevett</b>
            </td>
            <td style="text-align:right">
                <a href="../index.html">Home</a>
            </td>
        </table>
        <hr>
        <!-- end header -->

        <h1>Deep Learning Rant</h1>

        <h2>Reproducability</h2>

        <p>It is well known that deep learning has a reproducability problem. This should not be happening.</p>

        <p>For you to publish a deep learning paper you have to actually go out and code up your data handling, your algorithm and training loop. You've already done all of the work, why can't you also publish it? And why should anyone believe your results if they aren't reproducable?</p>

        <p>A common excuse is that the code is too integrated into the company's secret internal code. However, most deep learning papers are simple additions to existing algorithms, most of which most of which will already have open source implementations - just add your novel step on to that.</p>

        <p>If I cannot see your code, how can I verify your results? How do I know your evaluation script is correct? Maybe you've got a bug that benefits you, but if that's the case then it's probably in your best interest to keep it quiet.</p>

        <p>Luckily, these type of things seem to sort themselves out. If your results aren't independently reproducable then nobody will build on your work and it will - thankfully - fade into obscurity.</p>

        <h2>Hyperparameters</h2>

        <p>A common sight in the <i>experiments</i> section of deep learning papers is a list of all the hyperparameters used. Here's a relatively tame example: "we use an embedding dimension of 128, our LSTM has a 256 dimensional hidden state, followed by a 128 dimensional linear layer. We used Adam with a learning rate of 3e-4. The batch size was set to 256."</p>

        <p>It is obviously a good thing that papers include this stuff - even though they shouldn't be praised for doing it as it should be mandatory. However, it is very rare that papers state why they arrived at these values.</p>

        <p>The ones used in the example are relatively standard hyperparameters, but what if the authors used some very esoteric learning rate schedule? Does the model train without this scheduler? Why? Did you try an LSTM with a 128 dimensional hidden state? 512 dimensions? 1024? Why not? Why did you initialize your parameters using this initialization scheme instead of another?</p>

        <p>It is absolutely fine to say that you took the hyperparameters from another paper. That's the best starting point. But at least mention other hyperparameters you tried.</p>

        <p>If you found the hyperparameters via a parameter sweep, then you should say so and also state the range and the number of runs.</p>

        <p>And although not strictly related to hyperparameters, your results should be over multiple runs along with error bars. Especially in reinforcement learning where models are exceptionally sensitive to the random seed.</p>

        <h2>Exceptions</h2>

        <p>As always, there are exceptions to the rule. I don't expect DeepMind to fully open-source AlphaStar or for OpenAI to have done an extensive hyperparameter sweep on GPT-3.</p>

        <p>However, AlphaStar has actually been played against - and it was livestreamed! If you have access you can play with GPT-3 right now! Sure, there might have been a bug in GPT-3's evaluation script and the actual perplexity is 10% higher, but so what? I can't see the code but at least I can verify it works better than any other language model out there so I'm pretty sure they're not cheating.</p>

        <p>Is this how it's going to be from now on though? Large scale models can only be verified by a PR stunt or hype or how much money they make? It sure seems that way and I don't see any solution.</p>

        <br>
        <!-- begin footer -->
        <hr>
        <table>
            <td style="text-align:right">
                <a href="#">Top</a>
            </td>
        </table>
        <!-- end footer -->
    </body>
</html>